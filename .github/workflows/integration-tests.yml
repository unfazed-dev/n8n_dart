name: Integration Tests

on:
  # Run on pull requests
  pull_request:
    branches: [master, main, develop]
    paths:
      - 'lib/**'
      - 'test/integration/**'
      - '.github/workflows/integration-tests.yml'

  # Run on pushes to main branches
  push:
    branches: [master, main]

  # Allow manual trigger
  workflow_dispatch:
    inputs:
      skip_slow_tests:
        description: 'Skip slow tests'
        required: false
        type: boolean
        default: false

  # Run nightly for continuous validation
  schedule:
    - cron: '0 2 * * *' # Run at 2 AM UTC daily

env:
  # Dart version
  DART_VERSION: '3.2.0'

  # Test configuration
  TEST_TIMEOUT_SECONDS: 300
  TEST_MAX_RETRIES: 3
  TEST_POLLING_INTERVAL_MS: 2000

  # CI/CD configuration
  CI_RUN_INTEGRATION_TESTS: true
  CI_SKIP_SLOW_TESTS: ${{ github.event.inputs.skip_slow_tests || 'false' }}

jobs:
  # Unit tests first (fast feedback)
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Dart
        uses: dart-lang/setup-dart@v1
        with:
          sdk: ${{ env.DART_VERSION }}

      - name: Get dependencies
        run: dart pub get

      - name: Verify formatting
        run: dart format --output=none --set-exit-if-changed .

      - name: Analyze code
        run: dart analyze --fatal-infos

      - name: Run unit tests
        run: dart test --exclude-tags=integration --coverage=coverage

      - name: Upload coverage
        uses: codecov/codecov-action@v4
        with:
          files: ./coverage/lcov.info
          flags: unittests
          name: unit-tests-coverage

  # Integration tests (requires n8n cloud)
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: unit-tests # Only run if unit tests pass

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Dart
        uses: dart-lang/setup-dart@v1
        with:
          sdk: ${{ env.DART_VERSION }}

      - name: Get dependencies
        run: dart pub get

      - name: Validate test configuration
        run: |
          dart run test/integration/utils/validate_environment.dart
        env:
          N8N_BASE_URL: ${{ secrets.N8N_BASE_URL }}
          N8N_API_KEY: ${{ secrets.N8N_API_KEY }}

      - name: Check workflow availability
        run: |
          dart run test/integration/utils/verify_workflows.dart
        env:
          N8N_BASE_URL: ${{ secrets.N8N_BASE_URL }}
          N8N_API_KEY: ${{ secrets.N8N_API_KEY }}

      - name: Run integration tests
        id: integration_tests
        run: |
          # Start performance tracking
          START_TIME=$(date +%s)

          # Run tests with tags
          dart test \
            --tags=integration \
            --reporter=json \
            --file-reporter="json:test-results.json" \
            --coverage=coverage/integration \
            test/integration/

          # Calculate execution time
          END_TIME=$(date +%s)
          DURATION=$((END_TIME - START_TIME))
          echo "execution_time=$DURATION" >> $GITHUB_OUTPUT

          # Save metrics
          echo "{\"execution_time_seconds\": $DURATION, \"timestamp\": \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\"}" > test-metrics.json
        env:
          N8N_BASE_URL: ${{ secrets.N8N_BASE_URL }}
          N8N_API_KEY: ${{ secrets.N8N_API_KEY }}
          N8N_SIMPLE_WORKFLOW_ID: ${{ secrets.N8N_SIMPLE_WORKFLOW_ID || 'auto' }}
          N8N_WAIT_NODE_WORKFLOW_ID: ${{ secrets.N8N_WAIT_NODE_WORKFLOW_ID || 'auto' }}
          N8N_SLOW_WORKFLOW_ID: ${{ secrets.N8N_SLOW_WORKFLOW_ID || 'auto' }}
          N8N_ERROR_WORKFLOW_ID: ${{ secrets.N8N_ERROR_WORKFLOW_ID || 'auto' }}
          N8N_SIMPLE_WEBHOOK_PATH: 'test/simple'
          N8N_WAIT_NODE_WEBHOOK_PATH: 'test/wait-node'
          N8N_SLOW_WEBHOOK_PATH: 'test/slow'
          N8N_ERROR_WEBHOOK_PATH: 'test/error'

      - name: Generate HTML test report
        if: always()
        run: |
          dart run test/integration/utils/generate_report.dart test-results.json test-report.html

      - name: Upload test report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: integration-test-report
          path: |
            test-report.html
            test-results.json
            test-metrics.json
          retention-days: 30

      - name: Upload coverage
        uses: codecov/codecov-action@v4
        with:
          files: ./coverage/integration/lcov.info
          flags: integration
          name: integration-tests-coverage

      - name: Track performance metrics
        if: always()
        run: |
          echo "üìä Integration Test Metrics"
          echo "Execution time: ${{ steps.integration_tests.outputs.execution_time }} seconds"
          echo "Target: < 1200 seconds (20 minutes)"

          # Check if performance degraded
          if [ ${{ steps.integration_tests.outputs.execution_time }} -gt 1200 ]; then
            echo "‚ö†Ô∏è Warning: Tests took longer than 20 minutes"
          fi

      - name: Post PR comment
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');

            // Read test results
            let summary = '## üß™ Integration Test Results\n\n';

            try {
              const metrics = JSON.parse(fs.readFileSync('test-metrics.json', 'utf8'));
              const executionTime = metrics.execution_time_seconds;
              const minutes = Math.floor(executionTime / 60);
              const seconds = executionTime % 60;

              summary += `‚è±Ô∏è **Execution Time:** ${minutes}m ${seconds}s\n`;
              summary += `üéØ **Target:** < 20 minutes\n\n`;

              if (executionTime > 1200) {
                summary += '‚ö†Ô∏è **Warning:** Tests exceeded 20-minute target\n\n';
              }

              // Add test results link
              summary += `üìÑ **Full Report:** Available in workflow artifacts\n`;

            } catch (error) {
              summary += `‚ö†Ô∏è Could not parse test metrics: ${error.message}\n`;
            }

            // Post comment
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summary
            });

      - name: Cleanup old test executions
        if: always()
        run: |
          dart run test/integration/utils/cleanup_executions.dart
        env:
          N8N_BASE_URL: ${{ secrets.N8N_BASE_URL }}
          N8N_API_KEY: ${{ secrets.N8N_API_KEY }}
          MAX_EXECUTIONS_AGE_DAYS: 7

  # Performance tracking
  performance-tracking:
    name: Performance Tracking
    runs-on: ubuntu-latest
    needs: integration-tests
    if: always()

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download test metrics
        uses: actions/download-artifact@v4
        with:
          name: integration-test-report

      - name: Track performance trends
        run: |
          # This would integrate with monitoring service
          # For now, just log metrics
          cat test-metrics.json

          echo "Performance metrics logged for trend analysis"
